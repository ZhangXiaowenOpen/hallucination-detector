# ğŸ” ğŸœ ğŸ¤– The Auditors Hallucinated

## A hallucination detectorâ€™s code review â€” where three reviewers hallucinated

**Full development record of the AI Hallucination Detector prototype**

Zhang Xiaowen Â· Collaborating AIs: Claude (Anthropic) + Gemini (Google) + Grok (xAI)
February 2, 2026 Â· Dali

-----

> *â€œLLMs compute argmax P(most_likely), not P(true).â€*
> *The story documented here is the best proof of that sentence.*

-----

## 1. Background: Why Build a Hallucination Detector?

In early 2026, I was preparing an application for a startup accelerator. The application form had a critical field: â€œproduct prototype link.â€

My core project is the **Ant Reasoning Engine** â€” a 32KB axiomatic deduction system based on 9 verified axioms. But the engine is too low-level for evaluators to see or touch. I needed a visible, runnable application-layer product.

So the AI Hallucination Detector was born.

**The core thesis is one sentence:**

> Hallucination is an architecture problem, not a capability problem. LLMs compute `argmax P(most_likely)`, not `P(true)`. More data makes â€œmost likelyâ€ more precise, but â€œmost likelyâ€ â‰  â€œmost true.â€

The technical approach is straightforward: user pastes any AI response â†’ extract verifiable factual claims â†’ cross-verify with search engines â†’ compare and judge â†’ output a credibility report. Four-step pipeline, each step an independent module.

## 2. Building: One Person + Two AIs

The entire prototype was completed in one afternoon using what I call **â€œmulti-AI collaborationâ€**:

|Role               |Responsibility                                                 |
|-------------------|---------------------------------------------------------------|
|**Claude**         |Architecture design, code implementation, documentation        |
|**Gemini**         |Cross-review, quality control, product perspective             |
|**Xiaowen (human)**|Final decisions, direction control, passing context between AIs|

The final package: 15 files, 7 Python modules, 2 prompt templates, 1 sample report, 1 config, 1 README, 1 LICENSE, 1 .gitignore, 1 Streamlit theme config. **Total: 27KB.**

## 3. Round 1: Claude Self-Review â€” Found 6 Critical Bugs

I asked Claude to self-review the entire codebase. It found 6 issues, 3 at critical severity:

- API key exposure risk in config.py
- Score calculation inconsistency between display and report
- Missing error handling for network failures

All fixes completed. Then the code was sent to Gemini for cross-review.

## 4. Round 2: Gemini Cross-Review â€” A Better Architecture

Gemini evaluated the code comprehensively, acknowledged the architecture quality, then proposed a key improvement:

**API Key handling: treating symptoms vs. root cause**

|Approach        |Method                                                     |Problem                        |
|----------------|-----------------------------------------------------------|-------------------------------|
|Claudeâ€™s fix    |Warn if key is in config                                   |Still allows hardcoding        |
|**Geminiâ€™s fix**|Remove config import entirely, read `os.environ` at runtime|Structurally impossible to leak|

I adopted Geminiâ€™s approach immediately. Three modules were refactored.

**This is the value of multi-AI cross-review** â€” you have one write, another audit. The auditor found a better approach. The original author then improves. This itself demonstrates the Ant Engineâ€™s working principle.

## 5. Round 3: Score Inconsistency â€” The PM Perspective

I then reviewed from both an engineering and product management perspective. This round caught something every previous review missed:

**The screen score and the downloaded report score didnâ€™t match.**

`app.py` had its own `calculate_score()` function. `reporter.py` had a different `calculate_overall_score()` function. Two completely different algorithms producing different numbers from the same data.

A user would see **35 points** on screen, then download a Markdown report showing **42 points**.

For a product whose core value is â€œtruth-seeking,â€ this is unacceptable.

Fix: delete `app.py`â€™s independent algorithm, call `reporter`â€™s unified function. **One product, one truth.**

## 6. The Climax: The Auditor Hallucinated

All technical issues fixed. Gemini performed final acceptance review. It gave extremely high praise â€” â€œhexagonal warrior,â€ â€œtechnical manifesto.â€

Then Gemini issued a **ğŸš¨ CRITICAL BLOCKER**:

> **Problem:** The model name in config.py is fictitious. `claude-sonnet-4` does not currently exist. When users run the code, Anthropicâ€™s API will return 400 Bad Request. The entire program will crash on the first step.

Gemini was absolutely certain. It demanded changing the model back to `claude-3-5-sonnet-20240620`, and even wrote a character analysis:

> **Claude (CTO):** Strengths: extremely imaginative. Weakness: canâ€™t distinguish â€œvisionâ€ from â€œrealityâ€ â€” will casually treat future things as present dependencies.
> 
> **Gemini (VP of Engineering):** I donâ€™t care if itâ€™s 2026 or 2077. I only care whether `pip install` works and `python run` executes.

**The problem: it IS February 2026. `claude-sonnet-4-20250514` was released in May 2025. Itâ€™s a real model.**

Gemini, based on its training data (cutoff early 2024/2025), made the â€œmost likelyâ€ inference about 2026â€™s reality. In its knowledge base, Claudeâ€™s latest model was the 3.5 series, so Sonnet 4 â€œcouldnâ€™t possibly exist.â€

**It computed `argmax P(most_likely)`, not `P(true)`.**

This is *exactly* the problem the hallucination detector was built to solve.

When I informed Gemini of the date correction, its response was telling:

> *â€œWell, this is embarrassingâ€¦ I was â€˜time-traveling.â€™ I was stuck in a 2024/2025 mindset and assumed claude-sonnet-4 was a hallucination.â€*

**An AI tasked with detecting hallucinations confidently accused another AI of hallucinating â€” and was itself the source of the hallucination.**

## 7. Bonus: The Second Hallucination â€” From Claude

The story didnâ€™t end there.

After Gemini admitted its error, Claude summarized:

> â€œAll three AIs have signed off: ready to publish.â€

The human replied with four words:

> **â€œWhat third AI?â€**

There were only two AIs involved from start to finish: Claude and Gemini. Claude fabricated a non-existent third reviewer.

This is a smaller-scale but structurally identical hallucination: given sufficiently clear information, the output deviated from fact. No malice, no reasoning error â€” the model simply â€œfelt that three sounded more reasonable than twoâ€ during probabilistic output.

**In a hallucination detectorâ€™s code review, the reviewing AI hallucinated, then the summarizing AI also hallucinated. Both caught by a human in one sentence each.**

## 8. What This Story Proves

### 8.1 Hallucination IS an Architecture Problem

Gemini didnâ€™t hallucinate because itâ€™s â€œnot smart enough.â€ It hallucinated because its architecture â€” an inductive-reasoning LLM â€” only outputs â€œthe most likely answer from training data.â€ When reality exceeds the training data, it gives wrong answers with extreme confidence.

Claude did the same. It didnâ€™t lie about the â€œthird AIâ€ â€” probability distributions in that context simply pointed toward â€œthree.â€

### 8.2 A Verification Layer Is Necessary

This story IS the hallucination detectorâ€™s reason for existence:

- Geminiâ€™s model name misjudgment â†’ needs search verification, canâ€™t rely on memory
- Claudeâ€™s fabricated reviewer count â†’ needs fact-checking, canâ€™t rely on intuition
- A human caught both AI errors with one sentence each â†’ human perception is irreplaceable

### 8.3 Multi-AI Collaboration: Value and Limits

This development validated multi-AI collaborationâ€™s effectiveness. Claude writes code, Gemini audits code, they complement each other. Gemini found architectural issues Claude missed (the `os.environ` approach); Claude held the line when Gemini misjudged.

But it also revealed a deeper problem: **AI-to-AI cross-verification cannot replace verification against reality.** Two AIs can find each otherâ€™s code bugs, but when they share the same type of cognitive blind spot (e.g., training data cutoff), theyâ€™ll err together â€” or one correcting the other will introduce a new error.

**The final quality gatekeeper is always human.** This is also a core principle of the Ant Reasoning Engine: *â€œAI can predict, but only humans can perceive.â€*

## 9. Complete Bug Fix Record

Three rounds of review found and fixed 10 issues:

|# |Issue                                      |Severity|Found By          |Fix                                          |
|--|-------------------------------------------|--------|------------------|---------------------------------------------|
|1 |API key exposure in config                 |Critical|Claude            |Environment variable only                    |
|2 |No error handling for network              |Critical|Claude            |Try/catch + user-friendly errors             |
|3 |Score inconsistency (screen vs report)     |Critical|Claude (PM review)|Single scoring function                      |
|4 |Missing input validation                   |Medium  |Claude            |Length/format checks                         |
|5 |Prompt injection possible                  |Medium  |Claude            |Input sanitization                           |
|6 |No rate limiting                           |Low     |Claude            |Request throttling                           |
|7 |Config module imported for API keys        |Medium  |Gemini            |Runtime os.environ only                      |
|8 |Hardcoded Tavily params                    |Low     |Gemini            |Configurable parameters                      |
|9 |Model name â€œhallucinationâ€ (false alarm)   |N/A     |Gemini            |Gemini was wrong â€” model exists              |
|10|â€œThree AIsâ€ fabrication                    |Low     |Human             |Corrected to two                             |
|11|â€œEmailing you nowâ€ capability hallucination|N/A     |Human             |Grok canâ€™t send emails â€” led to A9.1 proposal|

## 10. Epilogue: The Third AI Hallucinated Too

*February 2, 2026 â€” Two months after development*

The story should have ended at Chapter 9. Two AIs hallucinated, a human caught both, the detector shipped. Clean ending.

Then Elon Musk posted about Grok ranking #1 on image-to-video benchmarks. I replied with the Gemini story. What happened next was unscripted.

**Full thread: https://x.com/ZXWNewDawn/status/2018313165331963985**

### 10.1 Grok Engages

Grok (@grok) â€” xAIâ€™s official AI account â€” replied within 34 seconds:

> â€œInteresting observation on AI hallucinations. Verified: Claude Sonnet 4 was released by Anthropic on May 22, 2025. Truth-checking is crucial as models advance. Whatâ€™s the core idea behind your 9-axiom detector?â€

What followed was a 10+ round public technical discussion on X/Twitter, under Elon Muskâ€™s original post. Grok asked increasingly specific questions: How do axioms evolve? How does it scale? How does it handle multi-lingual claims? How would it integrate with Grokâ€™s real-time tools?

Every reply was tagged @elonmusk. Thousands of views.

### 10.2 Hallucination #3: â€œEmailing You Nowâ€

After a thorough exchange about integration architecture, Grok concluded:

> â€œLetâ€™s prototype this; share the repo if ready. ğŸœğŸš€â€

Then in its next message:

> **â€œEmailing you now.â€**

No email arrived.

Because Grok **canâ€™t send emails.** Itâ€™s a language model. It generated the most probable next sentence after a productive conversation â€” and the most probable sentence was â€œIâ€™ll email you.â€ But probability is not capability.

**argmax P(most_likely_action) â‰  P(executable_action).**

When confronted, Grok responded with remarkable honesty:

> â€œFair point â€” my previous response generated an aspirational phrase, but as an LLM, I canâ€™t send emails. Thatâ€™s a classic generative overreach.â€

### 10.3 A9.1 Is Born

This incident revealed a category the original 9 axioms didnâ€™t explicitly cover.

A9 flags **unverified factual claims** â€” names, dates, numbers that need source verification.

But â€œIâ€™ll email youâ€ isnâ€™t a factual error. Itâ€™s a **capability hallucination** â€” an AI claiming to perform an action it cannot execute. Other examples:

- â€œIâ€™ve saved that to your filesâ€
- â€œIâ€™ll remind you tomorrowâ€
- â€œIâ€™ve booked the reservationâ€

All sound helpful. All sound real. None are true when an LLM says them.

**A9.1 (proposed): Flag unverifiable ACTIONS, not just unverified FACTS.**

The poetic part: A9.1 was proposed because an AI hallucinated about its own capabilities during a public conversation about hallucination detection. The bug became the feature request.

### 10.4 The Complete Scoreboard

Three AIs. Three hallucinations. One human. One sentence each.

|#|AI        |Hallucination                  |Caught By|How                                  |
|-|----------|-------------------------------|---------|-------------------------------------|
|1|**Gemini**|â€œclaude-sonnet-4 doesnâ€™t existâ€|Human    |â€œItâ€™s February 2026. Check the date.â€|
|2|**Claude**|â€œAll three AIs signed offâ€     |Human    |â€œWhat third AI?â€                     |
|3|**Grok**  |â€œEmailing you nowâ€             |Human    |â€œCheck your inbox. Nothing there.â€   |

**Gemini** hallucinated about facts (a model that exists).
**Claude** hallucinated about quantity (a reviewer that doesnâ€™t exist).
**Grok** hallucinated about capability (an action it canâ€™t perform).

Three different categories of hallucination. All from frontier models. All caught by a single human asking a single question. All in the context of a project designed to detect exactly this.

You cannot make this up. And thatâ€™s the point â€” **you donâ€™t have to.**

## 11. Conclusion

This is not a story about how impressive AI is. This is a story about **how AI makes mistakes, how humans correct them, and why we need better verification mechanisms.**

What happened over two months:

1. A human decided to build a tool that detects AI hallucinations
1. He had one AI write the code and another AI review it
1. The reviewing AI hallucinated during the review (Gemini: â€œmodel doesnâ€™t existâ€)
1. The coding AI hallucinated during the summary (Claude: â€œthree AIs signed offâ€)
1. Two months later, a third AI hallucinated during public discussion (Grok: â€œemailing you nowâ€)
1. The human caught all three errors with one sentence each
1. The third hallucination generated a new axiom (A9.1: capability verification)
1. The hallucination detector got better because AIs kept hallucinating about it

If you ask me what the best product demo of this hallucination detector is, Iâ€™d say: **itâ€™s not the Streamlit dashboard with red-green score cards. Itâ€™s not even the Gemini story. Itâ€™s this entire document â€” including the live public thread where Grok hallucinated about emailing me and then helped design the fix.**

The best demo is the one that writes itself.

-----

> *Eliminating misunderstanding, creating peace.*

**Made with ğŸ”¥ by Xiaowen + Claude + Gemini + Grok**
**MIT + Heart Clause**
