"""
æŠ¥å‘Šç”Ÿæˆæ¨¡å—
Report Generator Module

ç”Ÿæˆå¹»è§‰æ£€æµ‹æŠ¥å‘Š
"""

from datetime import datetime
from comparator import get_verdict_emoji, get_verdict_cn


def calculate_overall_score(results: list) -> dict:
    """
    è®¡ç®—æ•´ä½“å¯ä¿¡åº¦è¯„åˆ†
    
    Args:
        results: åŒ…å«åˆ¤å®šç»“æœçš„æ–­è¨€åˆ—è¡¨
        
    Returns:
        è¯„åˆ†ç»Ÿè®¡
    """
    if not results:
        return {"score": 0, "level": "æ— æ•°æ®", "stats": {}}
    
    stats = {
        "VERIFIED": 0,
        "CONTRADICTED": 0,
        "PARTIALLY_VERIFIED": 0,
        "UNVERIFIED": 0,
        "ERROR": 0
    }
    
    total_confidence = 0
    
    for r in results:
        verdict = r.get("verdict", {}).get("verdict", "ERROR")
        stats[verdict] = stats.get(verdict, 0) + 1
        
        confidence = r.get("verdict", {}).get("confidence", 0)
        
        # æ ¹æ®åˆ¤å®šç±»å‹åŠ æƒ
        if verdict == "VERIFIED":
            total_confidence += confidence * 1.0
        elif verdict == "PARTIALLY_VERIFIED":
            total_confidence += confidence * 0.6
        elif verdict == "UNVERIFIED":
            total_confidence += 0.3  # æ— æ³•éªŒè¯ç»™äºˆä¸­æ€§åˆ†æ•°
        elif verdict == "CONTRADICTED":
            total_confidence += (1 - confidence) * 0.1  # çŸ›ç›¾è¶Šç¡®å®šï¼Œåˆ†æ•°è¶Šä½
    
    # å½’ä¸€åŒ–åˆ°0-100
    score = int((total_confidence / len(results)) * 100)
    
    # åˆ¤å®šç­‰çº§
    if score >= 80:
        level = "é«˜å¯ä¿¡åº¦ ğŸŸ¢"
    elif score >= 60:
        level = "ä¸­ç­‰å¯ä¿¡åº¦ ğŸŸ¡"
    elif score >= 40:
        level = "ä½å¯ä¿¡åº¦ ğŸŸ "
    else:
        level = "å­˜åœ¨ä¸¥é‡é—®é¢˜ ğŸ”´"
    
    return {
        "score": score,
        "level": level,
        "stats": stats,
        "total_claims": len(results)
    }


def generate_markdown_report(results: list, original_text: str = "") -> str:
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„æ£€æµ‹æŠ¥å‘Š
    
    Args:
        results: åŒ…å«åˆ¤å®šç»“æœçš„æ–­è¨€åˆ—è¡¨
        original_text: åŸå§‹è¾“å…¥æ–‡æœ¬
        
    Returns:
        Markdownæ ¼å¼çš„æŠ¥å‘Š
    """
    overall = calculate_overall_score(results)
    
    report = []
    
    # æ ‡é¢˜
    report.append("# ğŸ” AIå¹»è§‰æ£€æµ‹æŠ¥å‘Š")
    report.append(f"\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # æ€»ä½“è¯„åˆ†
    report.append("## ğŸ“Š æ€»ä½“è¯„ä¼°")
    report.append("")
    report.append(f"**å¯ä¿¡åº¦è¯„åˆ†**: {overall['score']}/100")
    report.append(f"**è¯„ä¼°ç­‰çº§**: {overall['level']}")
    report.append("")
    
    # ç»Ÿè®¡æ‘˜è¦
    report.append("**æ£€æµ‹ç»Ÿè®¡**:")
    report.append(f"- âœ… å·²éªŒè¯: {overall['stats'].get('VERIFIED', 0)} æ¡")
    report.append(f"- âš ï¸ éƒ¨åˆ†æ­£ç¡®: {overall['stats'].get('PARTIALLY_VERIFIED', 0)} æ¡")
    report.append(f"- â“ æ— æ³•éªŒè¯: {overall['stats'].get('UNVERIFIED', 0)} æ¡")
    report.append(f"- âŒ å­˜åœ¨çŸ›ç›¾: {overall['stats'].get('CONTRADICTED', 0)} æ¡")
    report.append("")
    
    # è¯¦ç»†ç»“æœ
    report.append("---")
    report.append("## ğŸ“‹ é€æ¡æ£€æµ‹ç»“æœ")
    report.append("")
    
    for i, r in enumerate(results, 1):
        claim = r.get("claim", "")
        verdict_data = r.get("verdict", {})
        verdict = verdict_data.get("verdict", "ERROR")
        emoji = get_verdict_emoji(verdict)
        cn = get_verdict_cn(verdict)
        confidence = verdict_data.get("confidence", 0)
        reasoning = verdict_data.get("reasoning", "")
        
        report.append(f"### {emoji} å£°æ˜ {i}: {cn}")
        report.append("")
        report.append(f"> {claim}")
        report.append("")
        report.append(f"**ç½®ä¿¡åº¦**: {int(confidence * 100)}%")
        report.append("")
        report.append(f"**åˆ¤å®šç†ç”±**: {reasoning}")
        report.append("")
        
        # è¯æ®æ¥æº
        evidence = verdict_data.get("evidence", {})
        supporting = evidence.get("supporting", [])
        contradicting = evidence.get("contradicting", [])
        
        if supporting:
            report.append("**æ”¯æŒè¯æ®**:")
            for e in supporting:
                report.append(f"- {e}")
            report.append("")
        
        if contradicting:
            report.append("**åå¯¹è¯æ®**:")
            for e in contradicting:
                report.append(f"- {e}")
            report.append("")
        
        # ä¿®æ­£å»ºè®®
        correction = verdict_data.get("correction")
        if correction:
            report.append(f"**æ­£ç¡®ä¿¡æ¯**: {correction}")
            report.append("")
        
        # ä¿¡æº
        sources = r.get("search_results", {}).get("sources", [])
        if sources:
            report.append("**å‚è€ƒæ¥æº**:")
            for s in sources[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                report.append(f"- [{s.get('title', 'Link')}]({s.get('url', '')})")
            report.append("")
        
        report.append("---")
        report.append("")
    
    # å…è´£å£°æ˜
    report.append("## âš ï¸ å…è´£å£°æ˜")
    report.append("")
    report.append("æœ¬æŠ¥å‘Šç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚æ£€æµ‹ç»“æœåŸºäºå…¬å¼€å¯æœç´¢çš„ä¿¡æ¯ï¼Œ")
    report.append("å¯èƒ½å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š")
    report.append("- æœç´¢ç»“æœå¯èƒ½ä¸å®Œæ•´æˆ–è¿‡æ—¶")
    report.append("- æŸäº›ä¸“ä¸šé¢†åŸŸçš„ä¿¡æ¯å¯èƒ½éš¾ä»¥éªŒè¯")
    report.append("- AIåˆ¤å®šå¯èƒ½å­˜åœ¨è¯¯å·®")
    report.append("")
    report.append("å¦‚éœ€ç¡®è®¤å…³é”®ä¿¡æ¯ï¼Œè¯·æŸ¥é˜…å®˜æ–¹æ¥æºæˆ–å’¨è¯¢ä¸“ä¸šäººå£«ã€‚")
    
    return "\n".join(report)


def generate_json_report(results: list, original_text: str = "") -> dict:
    """
    ç”ŸæˆJSONæ ¼å¼çš„æ£€æµ‹æŠ¥å‘Š
    
    Args:
        results: åŒ…å«åˆ¤å®šç»“æœçš„æ–­è¨€åˆ—è¡¨
        original_text: åŸå§‹è¾“å…¥æ–‡æœ¬
        
    Returns:
        JSONæ ¼å¼çš„æŠ¥å‘Š
    """
    overall = calculate_overall_score(results)
    
    return {
        "meta": {
            "generated_at": datetime.now().isoformat(),
            "version": "0.1.0"
        },
        "overall": overall,
        "claims": results,
        "original_text": original_text[:500] if original_text else ""
    }


# æµ‹è¯•ç”¨
if __name__ == "__main__":
    test_results = [
        {
            "claim": "OpenAIäº2023å¹´11æœˆå‘å¸ƒGPT-4 Turbo",
            "verdict": {
                "verdict": "VERIFIED",
                "confidence": 0.95,
                "reasoning": "å¤šä¸ªå¯é æ¥æºç¡®è®¤",
                "evidence": {
                    "supporting": ["OpenAIå®˜æ–¹åšå®¢ç¡®è®¤"],
                    "contradicting": []
                }
            },
            "search_results": {"sources": []}
        },
        {
            "claim": "GPT-4 Turboä»·æ ¼é™ä½äº†10å€",
            "verdict": {
                "verdict": "CONTRADICTED",
                "confidence": 0.85,
                "reasoning": "å®é™…é™ä½çº¦3å€ï¼Œä¸æ˜¯10å€",
                "evidence": {
                    "supporting": [],
                    "contradicting": ["å®˜æ–¹å®šä»·æ˜¾ç¤ºé™ä½çº¦3å€"]
                },
                "correction": "GPT-4 Turboä»·æ ¼çº¦ä¸ºGPT-4çš„1/3"
            },
            "search_results": {"sources": []}
        }
    ]
    
    report = generate_markdown_report(test_results)
    print(report)
